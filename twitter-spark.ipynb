{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b07d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/rathin/spark-3.0.3-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7853cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests, sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from operator import add\n",
    "from textblob import TextBlob\n",
    "import pickle\n",
    "from sentiment_model import TweetsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_classifier.sav'\n",
    "sentiment_classifier = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838acf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendTopWords(counts, url):\n",
    "    def takeAndSend(time, rdd):\n",
    "        if not rdd.isEmpty():\n",
    "            word_counts = rdd.take(10)\n",
    "\n",
    "            words = []\n",
    "            values = []\n",
    "\n",
    "            for (word, count) in word_counts:\n",
    "                words.append(word)\n",
    "                values.append(count)\n",
    "\n",
    "            json_data = {'words': str(words), 'counts': str(values)}\n",
    "            print(json_data)\n",
    "\n",
    "            response = requests.post(url, data=json_data)\n",
    "\n",
    "    counts.foreachRDD(takeAndSend)\n",
    "\n",
    "def sendTweetSentiments(sentiments, url):\n",
    "    def takeAndSend(time, rdd):\n",
    "        if not rdd.isEmpty():\n",
    "            (name, (total, (pos, neutral, neg))) = rdd.first()\n",
    "\n",
    "            json_data = {'positive': pos, 'neutral': neutral, 'negative': neg, 'total': total}\n",
    "            print(json_data)\n",
    "\n",
    "            response = requests.post(url, data=json_data)\n",
    "\n",
    "    sentiments.foreachRDD(takeAndSend)\n",
    "    \n",
    "def getSentiment(text):\n",
    "    #sent = TextBlob(text).sentiment.polarity\n",
    "    sent = sentiment_classifier.classify([text])\n",
    "\n",
    "    if sent[0] == 1:\n",
    "        return (1, 0, 0)\n",
    "#     elif sent == 0:\n",
    "#         return (0, 1, 0)\n",
    "    else:\n",
    "        return (0, 0, 1)\n",
    "    \n",
    "def sendTweetSentimentsFromStream(kvs, url):\n",
    "    sentiments = kvs.map(lambda x: json.loads(x)) \\\n",
    "                    .map(lambda json_object: (json_object[\"user\"][\"screen_name\"], json_object[\"text\"], getSentiment(json_object[\"text\"]))) \\\n",
    "                    .map(lambda kv: ('count', (1, kv[2]))) \\\n",
    "                    .reduceByKey(lambda a, b: (a[0] + b[0], (a[1][0] + b[1][0], a[1][1] + b[1][1], a[1][2] + b[1][2])))\n",
    "    sentiments.pprint()\n",
    "    sendTweetSentiments(sentiments, url)\n",
    "\n",
    "def sendTopHashtagsFromStream(kvs, url):\n",
    "    tweets = kvs.map(lambda x: json.loads(x)) \\\n",
    "                .map(lambda json_object: (json_object[\"user\"][\"screen_name\"], json_object[\"text\"]))\n",
    "\n",
    "    lines = tweets.flatMap(lambda line: line[1].split(\" \"))\n",
    "\n",
    "    ## This part does the hashtag count\n",
    "    hashtag_counts = lines.filter(lambda word: len(word) >= 2 and word[0] == '#') \\\n",
    "                          .map(lambda word: (word, 1)) \\\n",
    "                          .reduceByKey(add) \\\n",
    "                          .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending = False))\n",
    "    hashtag_counts.pprint()\n",
    "    sendTopWords(hashtag_counts, url)\n",
    "\n",
    "def sendTopWordsFromStream(kvs, url):\n",
    "    tweets = kvs.map(lambda x: json.loads(x)) \\\n",
    "                .map(lambda json_object: (json_object[\"user\"][\"screen_name\"], json_object[\"text\"]))\n",
    "\n",
    "    lines = tweets.flatMap(lambda line: line[1].split(\" \"))\n",
    "\n",
    "    ## This part does the word count\n",
    "    sw = stopwords.words('english')\n",
    "    sw.extend(['rt', 'https', 'http', 'coronavirus', 'covid19', 'covid-19'])\n",
    "\n",
    "    counts = lines.map(lambda word: word.strip().lower()) \\\n",
    "                  .filter(lambda word: word not in sw) \\\n",
    "                  .filter(lambda word: len(word) >= 2 and word[0] != '#' and word[0] != '@') \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .reduceByKey(add) \\\n",
    "                  .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending = False))\n",
    "    counts.pprint()\n",
    "    sendTopWords(counts, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fa627",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"tweetStream\")\n",
    "# Create a local StreamingContext with batch interval of 2 second\n",
    "ssc = StreamingContext(sc, 2)\n",
    "# Create a DStream that conencts to hostname:port\n",
    "kvs = ssc.socketTextStream(\"localhost\", 5555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83552379",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfe006",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'http://localhost:5000/real-time/'\n",
    "sendTopHashtagsFromStream(kvs, server + 'update_hashtagcounts')\n",
    "sendTopWordsFromStream(kvs, server + 'update_counts')\n",
    "sendTweetSentimentsFromStream(kvs, server + 'update_sentiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881619f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start computing\n",
    "ssc.start()        \n",
    "# Wait for termination\n",
    "ssc.awaitTermination()\n",
    "ssc.stop(stopGraceFully = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606b129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55096db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f647157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52004b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
